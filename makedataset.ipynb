{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking all data and creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing stuff\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all scenarios of window 16: 100%|██████████| 4/4 [00:00<00:00, 12.79it/s]\n",
      "Loading all scenarios of window 32: 100%|██████████| 4/4 [00:00<00:00,  8.50it/s]\n",
      "Loading all scenarios of window 5: 100%|██████████| 4/4 [00:00<00:00, 16.84it/s]\n",
      "C:\\Users\\eyaph\\AppData\\Local\\Temp\\ipykernel_15284\\4230357887.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test = np.array(data)\n"
     ]
    }
   ],
   "source": [
    "#read the data\n",
    "#format: holder = pd.read_csv(\"filepathname\")\n",
    "folders = [\"window 16\",\"window 32\",\"window 5\"]\n",
    "category = [\"distance2\",\"abrupt\",\"ideal\",\"obstacle\"]\n",
    "data = []\n",
    "for i in folders:\n",
    "    for j in tqdm(category, desc =f\"Loading all scenarios of {i}\"):\n",
    "        filenumber = 0\n",
    "        filepath = os.path.join(i,j,str(filenumber)+\".csv\")\n",
    "        while (os.path.isfile(filepath)):\n",
    "            reading = pd.read_csv(filepath)[['linkQuality','neighborLinkQuality','RSSI value', 'AVG RSSI value','Connected']]\n",
    "\n",
    "            reading['RSSI value'] = (reading['RSSI value']-reading['RSSI value'].mean())/reading['RSSI value'].std()\n",
    "            reading['AVG RSSI value'] = (reading['AVG RSSI value']-reading['AVG RSSI value'].mean())/reading['AVG RSSI value'].std()\n",
    "\n",
    "            data.append(reading)\n",
    "            filenumber += 1\n",
    "            filepath = os.path.join(i,j,str(filenumber)+\".csv\")\n",
    "\n",
    "test = np.array(data, dtype=\"float32\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linkQuality': 0,\n",
       " 'neighborLinkQuality': 1,\n",
       " 'RSSI value': 2,\n",
       " 'AVG RSSI value': 3,\n",
       " 'Connected': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = pd.read_csv(os.path.join(\"window 16\",\"distance2\",\"1.csv\"))[['linkQuality','neighborLinkQuality','RSSI value', 'AVG RSSI value','Connected']]\n",
    "column_indices = {name: i for i, name in enumerate(testdata.columns)}\n",
    "column_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifying the split window function\n",
    "\n",
    "\n",
    "def split_window(features,input_slice,labels_slice,label_columns,column_indices,input_width,label_width):\n",
    "    input_columns = ['linkQuality','neighborLinkQuality','RSSI value', 'AVG RSSI value']\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(features[input_slice])\n",
    "    inputs = tf.stack(\n",
    "            [inputs[ :,column_indices[name]] for name in input_columns],\n",
    "            axis=-1)\n",
    "    labels = tf.convert_to_tensor(features[labels_slice])\n",
    "    if label_columns is not None:\n",
    "        labels = tf.stack(\n",
    "            [labels[ :,column_indices[name]] for name in label_columns],\n",
    "            axis=-1)\n",
    "\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "\n",
    "    inputs.set_shape([input_width, None])\n",
    "    labels.set_shape( (label_width, None))\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 100%|██████████| 70/70 [00:25<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec=(TensorSpec(shape=(5, 4), dtype=tf.float32, name=None), TensorSpec(shape=(1, 1), dtype=tf.float32, name=None))>\n",
      "(TensorSpec(shape=(5, 4), dtype=tf.float32, name=None), TensorSpec(shape=(1, 1), dtype=tf.float32, name=None))\n",
      "Inputs shape (batch, time, features): (32, 5, 4)\n",
      "Labels shape (batch, time, features): (32, 1, 1)\n",
      "Inputs shape (batch, time, features): (32, 5, 4)\n",
      "Labels shape (batch, time, features): (32, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "def makedataset(features,total_window,input_slice,labels_slice,label_columns,column_indices,input_width,label_width,slide = 1,batch=32):\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in tqdm(features,\"Generating dataset\"):\n",
    "        sliding = 0\n",
    "        i = np.array(i,dtype=\"float32\")\n",
    "        while(i.shape[0]>(sliding + total_window)):\n",
    "            input, label= split_window(i[sliding:total_window+sliding],input_slice,labels_slice,label_columns,column_indices,input_width,label_width)\n",
    "            #we need some collection mechanism here\n",
    "            data.append(input)\n",
    "            labels.append(label)\n",
    "            sliding+=1\n",
    "    data = tf.convert_to_tensor(data)\n",
    "    label = tf.convert_to_tensor(labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data,label))\n",
    "    print(dataset)\n",
    "    test = dataset.batch(batch_size= 32, drop_remainder=True)\n",
    "    print(dataset.element_spec)\n",
    "    return test\n",
    "    \n",
    "            \n",
    "\n",
    "input_width = 5\n",
    "shift = 1\n",
    "total_window = input_width+shift\n",
    "label_width = 1\n",
    "input_slice = slice(0, input_width)\n",
    "label_start = total_window - label_width\n",
    "labels_slice = slice(label_start, None)\n",
    "label_columns = [\"Connected\"]\n",
    "finaltest = makedataset(test,total_window,input_slice,labels_slice,label_columns,column_indices,input_width,label_width)\n",
    "for example_inputs, example_labels in finaltest.take(2):\n",
    "    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "    print(f'Labels shape (batch, time, features): {example_labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(5, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 30\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience, verbose = 1,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  history = model.fit(window, epochs=MAX_EPOCHS, verbose = 1,\n",
    "                      validation_data=window.take(10),\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "231/231 [==============================] - 19s 12ms/step - loss: 0.7307 - mean_absolute_error: 0.7877 - val_loss: 0.1645 - val_mean_absolute_error: 0.3320\n",
      "Epoch 2/30\n",
      "231/231 [==============================] - 2s 8ms/step - loss: 0.0870 - mean_absolute_error: 0.2327 - val_loss: 0.0523 - val_mean_absolute_error: 0.1760\n",
      "Epoch 3/30\n",
      "231/231 [==============================] - 2s 7ms/step - loss: 0.0427 - mean_absolute_error: 0.1585 - val_loss: 0.0293 - val_mean_absolute_error: 0.1272\n",
      "Epoch 4/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 0.0245 - mean_absolute_error: 0.1151 - val_loss: 0.0165 - val_mean_absolute_error: 0.0911\n",
      "Epoch 5/30\n",
      "231/231 [==============================] - 2s 8ms/step - loss: 0.0138 - mean_absolute_error: 0.0817 - val_loss: 0.0091 - val_mean_absolute_error: 0.0645\n",
      "Epoch 6/30\n",
      "231/231 [==============================] - 2s 7ms/step - loss: 0.0077 - mean_absolute_error: 0.0580 - val_loss: 0.0052 - val_mean_absolute_error: 0.0472\n",
      "Epoch 7/30\n",
      "231/231 [==============================] - 2s 7ms/step - loss: 0.0046 - mean_absolute_error: 0.0435 - val_loss: 0.0032 - val_mean_absolute_error: 0.0368\n",
      "Epoch 8/30\n",
      "231/231 [==============================] - 2s 8ms/step - loss: 0.0031 - mean_absolute_error: 0.0351 - val_loss: 0.0023 - val_mean_absolute_error: 0.0311\n",
      "Epoch 9/30\n",
      "231/231 [==============================] - 2s 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0297 - val_loss: 0.0018 - val_mean_absolute_error: 0.0270\n",
      "Epoch 10/30\n",
      "231/231 [==============================] - 2s 8ms/step - loss: 0.0019 - mean_absolute_error: 0.0265 - val_loss: 0.0015 - val_mean_absolute_error: 0.0250\n",
      "Epoch 11/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 0.0016 - mean_absolute_error: 0.0244 - val_loss: 0.0013 - val_mean_absolute_error: 0.0231\n",
      "Epoch 12/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 0.0014 - mean_absolute_error: 0.0224 - val_loss: 0.0011 - val_mean_absolute_error: 0.0212\n",
      "Epoch 13/30\n",
      "231/231 [==============================] - 2s 7ms/step - loss: 0.0012 - mean_absolute_error: 0.0206 - val_loss: 9.7911e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 14/30\n",
      "231/231 [==============================] - 2s 7ms/step - loss: 0.0010 - mean_absolute_error: 0.0191 - val_loss: 8.5841e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 15/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 8.9860e-04 - mean_absolute_error: 0.0178 - val_loss: 7.5583e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 16/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 7.8076e-04 - mean_absolute_error: 0.0166 - val_loss: 6.6736e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 17/30\n",
      "231/231 [==============================] - 2s 7ms/step - loss: 6.7777e-04 - mean_absolute_error: 0.0155 - val_loss: 5.9034e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 18/30\n",
      "231/231 [==============================] - 1s 5ms/step - loss: 5.8676e-04 - mean_absolute_error: 0.0145 - val_loss: 5.2297e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 19/30\n",
      "231/231 [==============================] - 1s 5ms/step - loss: 5.0582e-04 - mean_absolute_error: 0.0135 - val_loss: 4.6386e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 20/30\n",
      "231/231 [==============================] - 1s 5ms/step - loss: 4.3378e-04 - mean_absolute_error: 0.0125 - val_loss: 4.1179e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 21/30\n",
      "231/231 [==============================] - 2s 8ms/step - loss: 3.6994e-04 - mean_absolute_error: 0.0116 - val_loss: 3.6564e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 22/30\n",
      "231/231 [==============================] - 2s 8ms/step - loss: 3.1389e-04 - mean_absolute_error: 0.0107 - val_loss: 3.2441e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 23/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 2.6530e-04 - mean_absolute_error: 0.0099 - val_loss: 2.8735e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 24/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 2.2382e-04 - mean_absolute_error: 0.0091 - val_loss: 2.5398e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 25/30\n",
      "231/231 [==============================] - 1s 5ms/step - loss: 1.8899e-04 - mean_absolute_error: 0.0084 - val_loss: 2.2402e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 26/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 1.6017e-04 - mean_absolute_error: 0.0079 - val_loss: 1.9721e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 27/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 1.3656e-04 - mean_absolute_error: 0.0074 - val_loss: 1.7320e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 28/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 1.1726e-04 - mean_absolute_error: 0.0069 - val_loss: 1.5162e-04 - val_mean_absolute_error: 0.0068\n",
      "Epoch 29/30\n",
      "231/231 [==============================] - 1s 5ms/step - loss: 1.0139e-04 - mean_absolute_error: 0.0066 - val_loss: 1.3220e-04 - val_mean_absolute_error: 0.0064\n",
      "Epoch 30/30\n",
      "231/231 [==============================] - 1s 6ms/step - loss: 8.8181e-05 - mean_absolute_error: 0.0062 - val_loss: 1.1488e-04 - val_mean_absolute_error: 0.0059\n"
     ]
    }
   ],
   "source": [
    "history = compile_and_fit(lstm_model, finaltest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 7.6380e-05 - mean_absolute_error: 0.0057\n",
      "[7.637973612872884e-05, 0.005654463544487953]\n"
     ]
    }
   ],
   "source": [
    "testprediction = lstm_model.evaluate(finaltest.take(50))\n",
    "print(testprediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5028f67c6d18250ee72baa53996febd7bcc446cc51a97688acb305be1ee1e5b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
